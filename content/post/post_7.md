---
title: Research Proposal
date: 2023-05-04T05:14-5:00
---
    
    For my senior research project, I plan to create a parser that is meant to go through reflections of lab projects in the Allegheny Computer Science Department and provide suggestions related to quality of writing, clarity of ideas, and grammar and spelling errors. The suggestions offered will allow for the development and better understanding of technical writing in the Allegheny Computer Science Department. Implementing this project will require the use of the Natural Language Processing tool known as word2vec, which will use neural networks in order to convert textual data into numerical data to allow for its manipulation. When first thinking about the development of my senior research project, I was unsure what exactly I wanted to do. I started thinking about what I could possibly do, and I ended up deciding on building a tool. I believed that a tool would be the most interesting project to develop and would help me understand how tool building works. I decided to start looking into tools I can build with a focus on tools related to writing. Because of the fact that I am a double major I decided that writing should be something I focus on in order to reach the requirements for both majors. Once I had that initial brainstorming out of the way, I came up with the idea of making it something related to technical writing. I had discovered technical writing when doing some research into computer science careers. I found technical writing and thought that it was a perfect mix of writing and computer science, and so I should invest in learning more about it. I thought about the presence of technical writing in our Computer Science Department and realized that it's presence isn’t very prominent. Technical writing is done when there is a lot to be kept track of, so it makes sense that our study of documentation wouldn’t be something that goes too in depth in the department, as we never created anything that was so confusingly large. However, I realized that there is at least a bit of documentation practices that we take part in in the department, that practice being reflections. After brainstorming and having all these parts together I figured I could use all the ideas I thought about  in order to build upon the importance of technical writing. Through this process I arrived at my idea. In order to better the technical writing of the Allegheny Computer Science Department, and learn more about technical writing myself, I decided to create the parser for technical writing in the department. After creating the idea came the hard part, which was understanding how I could possibly go about implementing it. I first had to research how my idea could possibly work. Since my idea was like a spelling check I figured that I would need to go down either two different paths. I believed that the two options available to me would have to be developing an algorithm that would look through reflections and offer suggestions or training an AI model in order to do the same thing the algorithm would do. This is where my gap arised; I did not have the information to be able to develop either of these projects and would need to do further research in order to successfully say I have a method of creating my idea. When thinking about the development of the algorithm I would need to write, I realized that it would probably end up being extremely difficult to write. When it comes to language there is an enormous amount of spelling and grammar conventions to understand alone. Pairing that with sentence structure and conventions, and then sentence analysis and understanding, the size of the algorithm would be massive. What would end up happening would be that I wouldn’t be able to finish, and I would run into a mass amount of problems during development. When thinking about using AI I realized that I would probably need to gain a greater understanding of how AI works in order to actually create it. After gaining that understanding it would be a matter of feeding the model a lot of information in order to train it to do its intended purpose. This idea seemed more viable, however there was also the fact that it wasn’t a concise thought. I don’t think there is an overarching AI model that I can simply get and use, it would be a matter of finding how to get access to an AI model, it’s level of complexity, its version, etc.. and then training it in a way where it does certain things, but also doesn’t do certain things. In theory it was a good idea, but I knew that there’d be a lot more in between the process of creation and development. At both ends I felt as though I would end up in a position where I could possibly get lost in development and not have a good roadmap for actually accomplishing my goal. Natural Language Processing was how I ended up addressing my gap. I discovered Natural Language Processing when looking through previous senior research projects from long graduated seniors. The senior’s project I looked at was related to the limitations of Natural Language Processing. That specific project interested me because the phrase Natural Language Processing sounded like exactly what I would need in order to develop my project. From reading the senior research project I saw that the senior essentially was asking whether or not things like grammar and spelling checkers were correct as he doubted the capabilities of Natural Language Processing. He had written the project almost 12 years ago, meaning that things like grammar and spelling checks may not have been as advanced as it is now, thus highlighting how valid his question was for the technology of the time. However, he found out that the grammar and spelling checkers were more advanced than he realized, as they caught a lot of errors he expected them to skip over. The fact that Natural Language Processors were beating expectations 12 years in the past gave me a good feeling that with their constant development, they must have greatly advanced to have more uses and be even more helpful. From reading his research project and learning the term Natural Language Processing, I was able to go about researching better and finding a source that was very great in my development. After learning about Natural Language Processing, I decided to see if I can find a programming language to do it in. I decided to go with Python, since that is the language that I am most proficient in. From simply typing in the phrase “Natural Language Processing in Python” I found the book called Python Natural Language Processing by Jalaj Thanki (It was quite a funny discovery). In the book, Thanki discusses Natural Language Processing and the possibilities that can be achieved with it. Natural Language Processing allows a variety of analysis of words, sentences, languages, etc..  From skimming through the book I was able to ascertain that I would be able to develop my project, as Natural Language Processing is used for things like offering suggestions on work and going through work for grammar and spelling. Discovering this book allowed me to clear my gap and learn more about the project I was planning to develop. As I continued reading I discovered the tool that would end up being what I plan to use for the development of my project; word2vec. When first reading about word2vec I was slightly confused because it seemed to be a very complicated tool, but I was prepared to learn how to use it based on its potential. Word2vec had the capacity to do things like spelling and grammar checks, breaking apart words, breaking apart sentences, predicting words based on patterns, indicating words that were similar in a large dataset, and much more. It seemed like an extremely versatile tool that I would be able to use in order to develop my project. The confusion for the tool came from the idea of neural networking, which is the main function of how word2vec works. Word2vec works by using a two-layer neural network, meaning that there are two layers, the input layer and the output layer, and the hidden layer which can be called the manipulation layer. When simplifying the concept as so it is easy to understand that the input later takes inputs, runs through the hidden layer where it is manipulated, and outputs through the output layer. After discovering word2vec I continued my research into it in order to get more information from other sources that weren’t just mainly the book I had found. From my research on how word2vec works and how it can be used, I came across someone who provided a great example of this. I ended up finding Allison Parrish and her use of word2vec. Parrish is a software engineer and also a poet, so she demonstrated how word2vec could be used creatively for things like writing a poem on her GitHub repository. That very same repository also has information on how to start using word2vec for development. With the help of Thanki and Parrish I was able to learn a lot about word2vec and begin working on a prototype to show off word2vec. When originally creating a prototype I wanted to create something that would use word2vec in a simple way, however that soon changed. Word2vec is a complicated tool to use, and with the time given I would not be able to produce something that I could possibly show off, and so I decided to focus solely on showing off the software itself. In Thanki’s book, she shows many different examples of uses of Natural Language Processing, which included uses of word2vec as well. She showed off a very popular parser that utilized word2vec called the Stanford Parser. I started playing around with the parser and testing the different analysis techniques it had available for user manipulation. Testing and experimenting with the parser allowed me to understand its ability to break down sentences and words, and even derive sentiments based on sentences. I started to focus on the sentiment analysis part of the parser because I noticed that the sentiment analysis wasn’t exactly biased. I noticed that the analysis would declare sentiments based on inflammatory words, meaning that sentences that have inflammatory words were declared negative, when they weren’t exactly negative. I felt that this was important because it highlighted that the analysis that the parser would use may not always be correct. When thinking about ethical concerns related to my project, there isn’t really anything I can think about, but I can speculate possible concerns that may happen during development. Since my project is related to the quality of the writing, the parser may ask to express more that is necessary. For example, if a person is talking about something they are struggling with and the parser doesn’t understand that it should be focusing more on better communicating the struggle, and is instead focusing on how they can possibly fix the struggle, that would be something that is ethically wrong. I want the parser to focus on the message being conveyed and its clarity and not the actual message itself. In order to reach the conclusion I want to reach by the end of the semester next year, I must continue my research and understanding of word2vec. Being that it is the main tool that I plan to use, I must gain better proficiency in using it, so that I am able to develop my tool in the best way possible. I plan to continue reading Thanki’s book and looking into other resources that will allow me to get started in development. I will also need to develop a style guide that the word2vec tool will use to reference when parsing through work. With these developments I will be able to create my tool.